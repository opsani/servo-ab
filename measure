#!/usr/bin/env python3

from measure import Measure, ST_FAILED

import sys
import os
import time
import subprocess
import re
import json

from threading import Timer

DESC="Apache Benchmark measure driver for Opsani Optune"
VERSION="1.0.0"
HAS_CANCEL=True
PROGRESS_INTERVAL=30

DFLT_LOAD_CFG = {
    'n_threads'     : 10,
    'n_requests'    : 10000000,   # request limit
    't_limit'       : 180,         # time limit in seconds:   0 => no time limit
    't_warmup'      :  30,         # warmup time in seconds:  0 => no warmup
    'user'          : '',
    'password'      : '',
    'headers'       : [],
}

METRICS = {
    #FIXME: name metric names good identifiers (no spaces/special chars)
    'requests throughput': {
        'unit': 'requests/second',
    },
    'time taken': {
        'unit': 'seconds',
    },
    'number of errors': {
        'unit': 'count',
    },
    'time per request (across all concurrent requests)': {
        'unit': 'milliseconds',
    },
    'time per request': {
        'unit': 'milliseconds',
    },
}

class AB(Measure):
    # def __init__(self, version, cli_desc, supports_cancel, progress_interval):
    #     super().__init__(version, cli_desc, supports_cancel, progress_interval)

    # overwrites super
    def describe(self):
        return METRICS

    # overwrites super
    def handle_cancel(self, signal, frame):
        err = "Exiting due to signal: %s"%signal
        self.print_measure_error(err, ST_FAILED)
        try:
            self.proc.terminate()
        except:
            pass
        sys.exit(3)

    # overwrites super
    def measure(self):
        load_cfg = DFLT_LOAD_CFG.copy()
        try:
            load = self.input_data.get('control', {}).get('load', {})
        except:
            raise Exception('Invalid control configuration format in input')

        # update the config dictionary with the test URL
        load_cfg['test_url'] = os.environ.get('AB_TEST_URL')
        # update the config dictionary with any headers, split into a list on comma
        if os.environ.get('AB_HEADERS'):
            load_cfg['headers'] = json.loads(os.environ.get('AB_HEADERS'))
        load_cfg.update(load) # will override test_url and headers if provided in load
        if not load_cfg['test_url']:
            raise Exception('Load configuration is missing a test_url')

        result, command = self._run_ab(
            test_url      = load_cfg['test_url'],
            headers       = load_cfg['headers'],
            n_threads     = load_cfg['n_threads'],
            n_requests    = load_cfg['n_requests'],
            t_limit       = load_cfg['t_limit'],
            t_warmup      = load_cfg['t_warmup'],
            user          = load_cfg['user'],
            password      = load_cfg['password'],
            )

        metrics = {}

        # TODO: if metrics were passed on input, only return those
        for metric_name in result.keys():
            metrics[metric_name] = METRICS[metric_name].copy()
            metrics[metric_name]['value'] = result[metric_name]

        annotations = {
            'command': command,
        }

        return (metrics, annotations)

    # helper:  run apache benchmark measurement
    def _run_ab(self, test_url, headers, n_threads, n_requests, t_limit, t_warmup, user, password):

        # process input
        if user or password:
            auth=['-A', user + ':' + password]
        else:
            auth=[]
        t_limit = int(t_limit)
        t_warmup = int(t_warmup)

        # execute warmup command, if any
        prog_coefficient = 1.0
        prog_start = 0.0
        if t_warmup > 0:

            # set progress coefficient for warmup:  if not determinable from
            # relative time limits, use 25% of overall progress for warmup
            if t_limit > 0:
                prog_coefficient = t_warmup / (t_warmup + t_limit)
            else:
                prog_coefficient = 0.25

            # construct and run warmup command
            n_warmup_requests = n_requests # assume warmup is less than duration requested
            cmd = ['ab', '-l', '-c', str(n_threads)]
            cmd.extend(['-t', str(t_warmup)])
            cmd.extend(['-n', str(n_warmup_requests)])
            if headers:
                for header in headers:
                    cmd.extend(['-H', header])
            cmd.extend(auth)
            cmd.append(test_url)
            self._run_ab_progress(cmd, n_warmup_requests, t_warmup, 0.0, prog_coefficient)

            # set progress start and coefficient for succeeding measure command
            prog_start = prog_coefficient * 100.0
            prog_coefficient = 1.0 - prog_coefficient

        # construct and run measurement command
        cmd = ['ab', '-l', '-c', str(n_threads)]
        if t_limit > 0:
            cmd.extend(['-t', str(t_limit)])
        cmd.extend(['-n', str(n_requests)])
        if headers:
            for header in headers:
                cmd.extend(['-H', header])
        cmd.extend(auth)
        cmd.append(test_url)
        self._run_ab_progress(cmd, n_requests, t_limit, prog_start, prog_coefficient)

        # parse results:  time taken excludes any warmup time (this can be changed)
        time_taken = None
        n_errors = None
        rps = None
        t_reqs = None
        t_reqs_all = None

        for line in self.proc.stdout.read().decode().split('\n'):

            m = re.search(r'Time taken for tests:\s+([\d\.]+)', line)
            if m:
                time_taken = float(m.group(1))
                continue

            m = re.search(r'Failed requests:\s+(\d+)', line)
            if m:
                n_errors = int(m.group(1))
                continue

            m = re.search(r'Requests per second:\s+([\d\.]+)', line)
            if m:
                rps = float(m.group(1))
                continue

            m = re.search(r'Time per request:\s+([\d\.]+).+\(mean\)', line)
            if m:
                t_reqs = float(m.group(1))
                continue

            m = re.search(r'Time per request:\s+([\d\.]+).+\(mean, across all concurrent requests\)', line)
            if m:
                t_reqs_all = float(m.group(1))
                continue

        assert time_taken != None, \
            "Failed to parse Apache Benchmark output: time taken"
        assert n_errors   != None, \
            "Failed to parse Apache Benchmark output: number of errors"
        assert rps        != None, \
            "Failed to parse Apache Benchmark output: requests per second"
        assert t_reqs_all != None, \
            "Failed to parse Apache Benchmark output: time per req (all)"
        assert t_reqs     != None, \
            "Failed to parse Apache Benchmark output: time per req"

        result = {
            'requests throughput': rps,
            'time taken': time_taken,
            'number of errors': n_errors,
            'time per request (across all concurrent requests)':  t_reqs_all,
            'time per request': t_reqs,
        }

        command = ' '.join(cmd)
        return (result, command)

    # helper:  execute apache benchmark command and print progress
    def _run_ab_progress(self, cmd, n_requests, t_limit, prog_start, prog_coefficient):
        self.debug("Running test command:", cmd)
        self.proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        # start progress for time limited ab command:  update every 5 seconds -
        # it is printed by default every 30 seconds
        if t_limit > 0:
            t_start = time.time()
            t = repeatingTimer(5, self._update_timed_progress, t_start, t_limit,
                prog_start, prog_coefficient)
            t.start()

        # use try-and-finally to ensure the timer, if any, is canceled and does
        # not block driver exit in case of an exception
        try:

            # process stderr
            stderr = []
            with self.proc.stderr:
                for line in iter(self.proc.stderr.readline, b''):
                    l = line.decode().strip()
                    stderr.append(l)

                    # glean progress from stderr where measure is not time limited
                    if t_limit <= 0:
                        m = re.search(r'Completed (\d+) requests', l)
                        if m:
                            prog = min(100.0, 100.0 * int(m.group(1)) / n_requests)
                            self.progress = min(100, int((prog_coefficient * prog) + prog_start))
                            self.print_progress()

            return_code = self.proc.wait()
        except Exception as e:
            raise
        finally:
            if t_limit > 0:
                t.cancel()

        if return_code != 0:
            self.debug("Command failed. Stderr:\n" + '\n'.join(stderr))
            raise Exception("Failed to measure, Apache Benchmark failed with exit code: " + str(return_code))

    # helper:  update timer based progress
    def _update_timed_progress(self, t_start, t_limit, prog_start, prog_coefficient):
        prog = min(100.0, 100.0 * (time.time() - t_start) / t_limit)
        self.progress = min(100, int((prog_coefficient * prog) + prog_start))


class repeatingTimer():
    def __init__(self, seconds, func, *args):
        self.seconds = seconds
        self.func = func
        self.args = args
        self.thread = Timer(self.seconds, self.call_func)
    def call_func(self):
        self.func(*self.args)
        self.thread = Timer(self.seconds, self.call_func)
        self.thread.start()
    def start(self):
        self.thread.start()
    def cancel(self):
        self.thread.cancel()


if __name__ == '__main__':
    ab = AB(VERSION, DESC, HAS_CANCEL, PROGRESS_INTERVAL)
    ab.run()
